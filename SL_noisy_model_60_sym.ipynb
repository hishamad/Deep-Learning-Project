{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4900,
     "status": "ok",
     "timestamp": 1715782928249,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "kyqOgTWzCTUX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 14:53:35.279471: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-22 14:53:35.315626: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-22 14:53:35.315662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-22 14:53:35.316589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-22 14:53:35.322656: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from cosine_annealing import CosineAnnealingScheduler\n",
    "from keras.layers import BatchNormalization\n",
    "import pickle\n",
    "from keras.regularizers import l2\n",
    "from  scipy import ndimage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y67Ec5FICTUd"
   },
   "source": [
    "#### Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYNU45pfCTUf"
   },
   "source": [
    "Load dataset + One-hot encoding + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1715782996141,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "o8BGn2ZOCTUg"
   },
   "outputs": [],
   "source": [
    "def load_dataset(noise_ratio=0, sym=True):\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    n_classes = 10\n",
    "\n",
    "    if noise_ratio > 0:\n",
    "        trainY_original = np.copy(trainY)\n",
    "        if sym:\n",
    "            # Symmetric noise\n",
    "            source_class = [9, 2, 3, 5, 4]\n",
    "            target_class = [1, 0, 5, 3, 7]\n",
    "            \n",
    "            for s, t in zip(source_class, target_class):\n",
    "                cls_idx = np.where(trainY_original == s)[0]\n",
    "                n_noisy = int(noise_ratio * cls_idx.shape[0] / 100)\n",
    "                noisy_sample_index = np.random.choice(cls_idx, n_noisy, replace=False)\n",
    "                trainY[noisy_sample_index] = t\n",
    "        else:\n",
    "            # Asymetric noise\n",
    "            n_samples = trainY.shape[0]\n",
    "            n_noisy = int(noise_ratio * n_samples / 100)\n",
    "            class_index = [np.where(trainY_original == i)[0] for i in range(n_classes)]\n",
    "            class_noisy = int(n_noisy / n_classes)\n",
    "\n",
    "            noisy_idx = []\n",
    "            for d in range(n_classes):\n",
    "                noisy_class_index = np.random.choice(class_index[d], class_noisy, replace=False)\n",
    "                noisy_idx.extend(noisy_class_index)\n",
    "\n",
    "            for i in noisy_idx:\n",
    "                class_ind = trainY_original[i]\n",
    "                other_class_list = list(range(n_classes))\n",
    "                other_class_list.remove(class_ind)\n",
    "                other_class = np.random.choice(other_class_list)\n",
    "                trainY[i] = other_class\n",
    "        \n",
    "    # One-hot encoding\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    # Normalize to [0,1]\n",
    "    trainX = trainX.astype('float32') / 255.0\n",
    "    testX = testX.astype('float32') / 255.0\n",
    "\n",
    "    train_mean = trainX.mean((0,1,2))\n",
    "    train_std = trainX.std((0,1,2))\n",
    "\n",
    "    trainX = (trainX - train_mean)# / train_std\n",
    "\n",
    "    test_mean = testX.mean((0,1,2))\n",
    "    test_std = testX.std((0,1,2))\n",
    "\n",
    "    testX = (testX - test_mean)# / test_std\n",
    "    \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eHdUCx4CTUg"
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZUdh8aqCTUh"
   },
   "source": [
    "Imortant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715784774519,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "BGpONL8FCTUh"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch == 30:\n",
    "        return lr / 10\n",
    "    elif epoch == 60:\n",
    "        return lr / 10\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "def train(model, trainX, trainY, testX, testY):\n",
    "\n",
    "\tgenerator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "\ttrain_data = generator.flow(trainX, trainY, batch_size=64)\n",
    "\n",
    "\tresults = model.fit(train_data, epochs=100, validation_data=(testX, testY), verbose=1, callbacks=[callback])\n",
    "\n",
    "\treturn results\n",
    "\n",
    "def evaluate(model, testX, testY):\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\n",
    "    print(f\"The model achieved a final accuracy of {acc*100:.2f}%\")\n",
    "\n",
    "def plot_results(history):\n",
    "\tplt.title('Cross Entropy Loss')\n",
    "\tplt.plot(history['loss'], color='blue', label='Training loss')\n",
    "\tplt.plot(history['val_loss'], color='orange', label='Validation/test loss')\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "\tplt.title('Classification Accuracy')\n",
    "\tplt.plot(history['accuracy'], color='blue', label='Training accuracy')\n",
    "\tplt.plot(history['val_accuracy'], color='orange', label='Validation/test accuracy')\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715783002196,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "Ls19xeC0CTUi"
   },
   "outputs": [],
   "source": [
    "def save_model(filename, model):\n",
    "    filename = './models/' + filename\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "def save_history(filename, history):\n",
    "    filename = './history/' + filename\n",
    "    pickle.dump(history, open(filename, 'wb'))\n",
    "\n",
    "def load_model(filename):\n",
    "    filename = './models/' + filename\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "    return model\n",
    "\n",
    "def load_history(filename):\n",
    "    filename = './history/' + filename\n",
    "    history = pickle.load(open(filename, 'rb'))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SL loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/YisenWang/symmetric_cross_entropy_for_noisy_labels/blob/master/loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_cross_entropy(y_true, y_pred):\n",
    "    alpha = 0.1\n",
    "    beta = 1.0\n",
    "    y_true_1 = y_true\n",
    "    y_pred_1 = y_pred\n",
    "\n",
    "    y_true_2 = y_true\n",
    "    y_pred_2 = y_pred\n",
    "\n",
    "    y_pred_1 = tf.clip_by_value(y_pred_1, 1e-7, 1.0)\n",
    "    y_true_2 = tf.clip_by_value(y_true_2, 1e-4, 1.0)\n",
    "\n",
    "    return alpha*tf.math.reduce_mean(-tf.math.reduce_sum(y_true_1 * tf.math.log(y_pred_1), axis = -1)) + beta*tf.math.reduce_mean(-tf.math.reduce_sum(y_pred_2 * tf.math.log(y_true_2), axis = -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqNA_VcCTUj"
   },
   "source": [
    "Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715789630290,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "C9xaw4BBCTUj"
   },
   "outputs": [],
   "source": [
    "def dropout_model():\n",
    "    # Create architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3), kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    total_steps = int(trainX.shape[0]/64) * 100\n",
    "    decay_steps = total_steps*0.8\n",
    "    initial_learning_rate = 0.01\n",
    "    warmup_steps = 1000\n",
    "    target_learning_rate = 0.1\n",
    "    alpha_2 = 0.001\n",
    "\n",
    "    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate, decay_steps ,alpha = alpha_2\n",
    "    )\n",
    "    # Optimization method\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Compile model and choose loss type\n",
    "    model.compile(optimizer=opt, loss=symmetric_cross_entropy, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9036,
     "status": "ok",
     "timestamp": 1715783015758,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "q_FZ9SNHCTUj",
    "outputId": "db089f9d-58a3-4b4e-c9d6-6d4208bdcd7f"
   },
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = load_dataset(noise_ratio=60, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3670727,
     "status": "ok",
     "timestamp": 1715793305366,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "5aSe_cO4CTUk",
    "outputId": "490b509f-4efa-4d66-cae6-6271c8a1ee29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 14:54:46.443031: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 9.0\n",
      "2024-05-22 14:54:46.443056: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2024-05-22 14:54:46.443108: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443155: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443180: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443218: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443333: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443455: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443552: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.443598: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.469431: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-05-22 14:54:46.544169: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-05-22 14:54:46.682334: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.682363: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.682387: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.682885: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.683624: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.685385: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.685401: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.685414: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.881971: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.884936: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.991109: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.991141: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.991163: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.991899: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.991919: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.993672: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.993688: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:46.993704: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.340817: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.456182: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.511323: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.548238: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.719508: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.719597: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.719781: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.720926: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.720946: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.720961: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.721702: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.728235: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:47.731875: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-22 14:54:48.085680: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fac5d267ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-22 14:54:48.085718: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA H100 80GB HBM3 MIG 1g.10gb, Compute Capability 9.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716389688.147433  216057 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 28s 28ms/step - loss: 6.0788 - accuracy: 0.4493 - val_loss: 7.2776 - val_accuracy: 0.3390 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 5.0776 - accuracy: 0.5529 - val_loss: 6.3406 - val_accuracy: 0.4185 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 4.7399 - accuracy: 0.5872 - val_loss: 5.9676 - val_accuracy: 0.4569 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 4.5159 - accuracy: 0.6093 - val_loss: 5.9734 - val_accuracy: 0.4585 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.4008 - accuracy: 0.6214 - val_loss: 6.0286 - val_accuracy: 0.4529 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.3022 - accuracy: 0.6330 - val_loss: 5.8660 - val_accuracy: 0.4687 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.2522 - accuracy: 0.6375 - val_loss: 5.7651 - val_accuracy: 0.4760 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.2100 - accuracy: 0.6426 - val_loss: 5.8550 - val_accuracy: 0.4723 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 4.1450 - accuracy: 0.6519 - val_loss: 5.8420 - val_accuracy: 0.4716 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 4.1109 - accuracy: 0.6554 - val_loss: 6.0581 - val_accuracy: 0.4538 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.0825 - accuracy: 0.6590 - val_loss: 5.7167 - val_accuracy: 0.4859 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.0511 - accuracy: 0.6630 - val_loss: 5.9305 - val_accuracy: 0.4689 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 4.0039 - accuracy: 0.6693 - val_loss: 5.7971 - val_accuracy: 0.4793 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9907 - accuracy: 0.6715 - val_loss: 5.7453 - val_accuracy: 0.4875 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9874 - accuracy: 0.6731 - val_loss: 5.6979 - val_accuracy: 0.4911 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.9543 - accuracy: 0.6777 - val_loss: 5.7265 - val_accuracy: 0.4880 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.9737 - accuracy: 0.6759 - val_loss: 5.6953 - val_accuracy: 0.4915 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9532 - accuracy: 0.6801 - val_loss: 5.7537 - val_accuracy: 0.4872 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9534 - accuracy: 0.6812 - val_loss: 5.8930 - val_accuracy: 0.4754 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.9451 - accuracy: 0.6828 - val_loss: 5.9786 - val_accuracy: 0.4723 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9500 - accuracy: 0.6836 - val_loss: 5.8432 - val_accuracy: 0.4832 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.9541 - accuracy: 0.6835 - val_loss: 5.8343 - val_accuracy: 0.4814 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.9126 - accuracy: 0.6876 - val_loss: 5.7794 - val_accuracy: 0.4871 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 3.9059 - accuracy: 0.6886 - val_loss: 6.1745 - val_accuracy: 0.4543 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.8999 - accuracy: 0.6903 - val_loss: 5.9098 - val_accuracy: 0.4788 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 3.9149 - accuracy: 0.6903 - val_loss: 5.9242 - val_accuracy: 0.4779 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9428 - accuracy: 0.6886 - val_loss: 5.7819 - val_accuracy: 0.4902 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 3.9427 - accuracy: 0.6902 - val_loss: 5.8513 - val_accuracy: 0.4910 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9331 - accuracy: 0.6924 - val_loss: 5.8339 - val_accuracy: 0.4935 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 3.9326 - accuracy: 0.6923 - val_loss: 5.8509 - val_accuracy: 0.4808 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "653/782 [========================>.....] - ETA: 2s - loss: 3.7544 - accuracy: 0.7108"
     ]
    }
   ],
   "source": [
    "model = dropout_model()\n",
    "results = train(model, trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "executionInfo": {
     "elapsed": 2168,
     "status": "ok",
     "timestamp": 1715793363574,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "yNdQN-nnCTUk",
    "outputId": "fc0a0793-fbbc-436a-b9b3-5941ca4db1ca"
   },
   "outputs": [],
   "source": [
    "plot_results(results.history)\n",
    "evaluate(model, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1715793629457,
     "user": {
      "displayName": "Jonatan Hyberg",
      "userId": "02103785444766429018"
     },
     "user_tz": -120
    },
    "id": "VmsurzVECTUk"
   },
   "outputs": [],
   "source": [
    "save_model('SL_noisy_model_60_sym', model)\n",
    "save_history('SL_noisy_model_60_sym', results.history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1irCtCvrPih104WASUJu-j-MZJMXLKCEy",
     "timestamp": 1715782757909
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63009f7f2cc996fccde2458c7f62140de231233c09c29bb8f0fa7c6864762ba3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
